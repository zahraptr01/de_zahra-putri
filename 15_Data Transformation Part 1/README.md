Rangkuman Materi Data Transformation
1.	Data transformation adalah proses mengkonversi data dari satu format atau struktur ke format atau struktur lainnya. 
2.	Data transformation penting untuk digunakan karena memungkinkan data dari berbagai sumber yang berbeda untuk digabungkan, memastikan kualitas dan konsistensi data, dan memfasilitasi analisis data dan kecerdasan bisnis.
3.	Ada beberapa jenis tranformasi data yang umum digunakan yaitu normalization (mengubah scaling data ke standard range), encoding (konversi categorical data ke numerical format), dan aggregation (merangkum data untuk dianalisis).
4.	Normalization bertujuan untuk memastikan setiap fitur akan berkontribusi secara setara pada perhitungan dan biasa digunakan pada algoritma machine learning. Metode yang sering digunakan adalah Min-Max scaling dan Z-score normalization.
5.	Encoding bertujuan untuk mengkonversi categorical data ke numerical data karena algoritma machine learning memerlukan input numerical. Metode yang sering digunakan adalah One-hot encoding dan Label encoding.
6.	Aggregation bertujuan untuk membantu data engineer menyediakan tampilan ringkasan dari data. Metode yang sering digunakan adalah sum, average, count, max, dan min.
7.	Handling missing values bertujuan untuk menangani data yang hilang karena berbagai alasan seperti kesalahan entry data atau data corruption. Ada tiga metod yang sering digunakan yaitu deletion (menghapus baris dari data yang hilang), imputation (mengisi data yang hilang berdasarkan data poin lainnya), dan forward/backward fill (menggunakan nilai sebelum/sesudahnya untuk mengisi nilai yang hilang).
8.	Outlier data adalah nilai yang sangat berbeda dengan nilai yang ada dalam dataset. Outlier muncul karena ada varibilitas data atau kesalahan. 
9.	Integrasi data memastikan bahwa data tetap akurat, konsisten, dan tidak berubah selama penyimpanan atau transfer. Menghindari kehilangan data sangat penting untuk menjaga kelengkapan kumpulan data dan memastikan analisis yang andal. Metode yang sering digunakan adalah regular backups (membuat Salinan kumpulan data secara berkala), validation checks (menerapkan aturan untuk memastikan data masuk ke dlaam sistem mematuhi kriteria yang telah ditentukan), dan checksums (menggunakan algoritma untuk memverifikasi integritas data selama transfer).
10.	ETL (Extract, Transform, Load) adalah proses memngumpulkan data dari berbagai sumber dan memodifikasi jika diperlukan.
11.	Data wrangling adalah proses membersihkan dan mengorganisir data yang berantakan agar cocok digunakan untuk analisis.
